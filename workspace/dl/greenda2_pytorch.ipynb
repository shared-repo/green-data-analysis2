{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"mps:0\" if torch.backends.mps.is_available() else \"cpu\") # for macos\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # for windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data-files/FashionMNIST\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data-files/FashionMNIST\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data-files/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data-files/FashionMNIST\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data-files/FashionMNIST\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data-files/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data-files/FashionMNIST\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data-files/FashionMNIST\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data-files/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data-files/FashionMNIST\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data-files/FashionMNIST\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data-files/FashionMNIST\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.FashionMNIST(\"data-files/FashionMNIST\", \n",
    "                                                  download=True, \n",
    "                                                  transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\"data-files/FashionMNIST\", \n",
    "                                                 download=True, train=False,\n",
    "                                                 transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset으로부터 데이터를 읽어서 모델에 전달하는 도구\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([1, 28, 28]) 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print( len(train_dataset[0]) )\n",
    "print( train_dataset[0][0].shape, train_dataset[0][1] )\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(train_dataset[3][0][0], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMnistDNN(nn.Module): # nn.Module : pytorch의 모델 기본형\n",
    "    def __init__(self):\n",
    "        # super.__init__()\n",
    "        super(FashionMnistDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(self, input_data): # 네트워크 실행시에 호출되는 함수\n",
    "        out = input_data.view(-1, 28*28)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMnistDNN(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "model = FashionMnistDNN()\n",
    "model.to(device) # 연산 메모리 영역에 변수 할당\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in np.arange(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        X, y = images.to(device), labels.to(device) # 연산 메모리 영역에 변수 할당\n",
    "\n",
    "        outputs = model(X)\n",
    "        l = loss(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step() # 가중치 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy : 0.877299964427948\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "total_count = 0\n",
    "for images, labels in test_loader:\n",
    "    X, y = images.to(device), labels.to(device) # 연산 메모리 영역에 변수 할당\n",
    "    outputs = model(X)\n",
    "\n",
    "    # print( torch.max(outputs, 1)[1] ) # 예측 값\n",
    "    predictions = torch.max(outputs, 1)[1]\n",
    "    # print(y)\n",
    "    # print(predictions == y)\n",
    "    # print( (predictions == y).sum() )\n",
    "    correct_count += (predictions == y).sum()\n",
    "    total_count += len(y)\n",
    "\n",
    "# outputs.shape, outputs[:3]\n",
    "    \n",
    "print(f\"test accuracy : {correct_count / total_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
