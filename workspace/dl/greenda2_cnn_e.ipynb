{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P4QWNlE53jaU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as tf_keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "cp drive/MyDrive/Colab\\ Notebooks/data-files/dogs-vs-cats.zip sample_data/\n",
        "unzip sample_data/dogs-vs-cats.zip -d sample_data\n",
        "unzip sample_data/dogs-vs-cats/train.zip -d sample_data/dogs-vs-cats\n",
        "unzip sample_data/dogs-vs-cats/test1.zip -d sample_data/dogs-vs-cats\n",
        "mv sample_data/dogs-vs-cats/test1 sample_data/dogs-vs-cats/test\n",
        "rm -rf sample_data/__MACOSX"
      ],
      "metadata": {
        "id": "JtljH31CAcD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "src_base = 'sample_data/dogs-vs-cats'\n",
        "dest_base = 'sample_data/dogs-vs-cats2'\n",
        "\n",
        "if os.path.exists(dest_base): # 경로 존재여부 확인\n",
        "  shutil.rmtree(dest_base) # train 경로 및 하위 경로 삭제\n",
        "os.mkdir(dest_base) # 디렉터리 만들기\n",
        "\n",
        "# train 폴더의 0 ~ 1000 : train, 1000 ~ 1500 : validation, 1500 ~ 2000 : test 세트로 구성\n",
        "for start, stop, path in zip([0, 1000, 1500], [1000, 1500, 2000], ['train', 'validation', 'test']):\n",
        "  os.mkdir(os.path.join(dest_base, path))\n",
        "  for sub_path in ['cat', 'dog']:\n",
        "    os.mkdir(os.path.join(dest_base, path, sub_path))\n",
        "    for idx in np.arange(start, stop):\n",
        "      fname = f'{sub_path}.{idx}.jpg'\n",
        "      shutil.copy(os.path.join(src_base, 'train', fname), os.path.join(dest_base, path, sub_path, fname)) # 파일 복사"
      ],
      "metadata": {
        "id": "ZX8kSSc0NT0Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 자동화 도구 만들기 : 파일을 읽어서 모델에 입력 가능한 형식으로 변환하는 도구\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_generator = ImageDataGenerator(rescale=1/255) # 특정 디렉터리의 파일을 읽어서 모델의 입력데이터로 변환하는 도구\n",
        "validation_data_generator = ImageDataGenerator(rescale=1/255)\n",
        "test_data_generator = ImageDataGenerator(rescale=1/255)"
      ],
      "metadata": {
        "id": "ig1ds40CiKtW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generator를 사용해서 파일 데이터 읽기\n",
        "datasets = []\n",
        "for path, generator in zip(['train', 'validation', 'test'],\n",
        "                           [train_data_generator, validation_data_generator, test_data_generator]):\n",
        "  dataset = generator.flow_from_directory(directory=f\"sample_data/dogs-vs-cats2/{path}\",\n",
        "                                          target_size=(256, 256),\n",
        "                                          batch_size=32,\n",
        "                                          class_mode=\"binary\")\n",
        "  datasets.append(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsEuBNIIjwQS",
        "outputId": "1e471056-a353-4138-c68f-baf62d55398c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미 학습 완료된 비슷한 모델 활용 ( 전이학습 )\n",
        "\n",
        "# 1. 다른 모델의 합성곱 층이 출력한 데이터를 현재 모델의 입력으로 사용\n",
        "# 2. 다른 모델과 현재 모델 결합 1 (다른 모델의 합성곱 층에 층을 추가해서 새 모델 구성 - 다른 모델의 합성곱 층은 학습하지 않음 )\n",
        "# 3. 다른 모델과 현재 모델 결합 2 (다른 모델의 합성곱 층에 층을 추가해서 새 모델 구성 - 다른 모델의 합성곱 층의 일부는 학습 )"
      ],
      "metadata": {
        "id": "ZeVdCMjfNORe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 학습 모델 준비\n",
        "\n",
        "base_model = tf_keras.applications.vgg16.VGG16(include_top=False,\n",
        "                                               weights='imagenet',\n",
        "                                               input_shape=(256, 256, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0dh5iEWPiL3",
        "outputId": "dfee5994-f976-4c89-b897-77a9e37deec7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS3XcREqRfzI",
        "outputId": "f886fc08-1eeb-447b-99cd-a9b7a8c2fe1c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 14714688 (56.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 학습 모델의 출력 테스트\n",
        "import cv2\n",
        "\n",
        "cat_img = cv2.imread('sample_data/dogs-vs-cats/train/cat.0.jpg', cv2.IMREAD_COLOR) # BRG\n",
        "cat_img = cv2.cvtColor(cat_img, cv2.COLOR_BGR2RGB) # BGR -> RGB\n",
        "cat_img = cv2.resize(cat_img, (256, 256))\n",
        "print(cat_img.shape)\n",
        "\n",
        "input = cat_img.reshape((-1, 256, 256, 3)) # 입력형식 : ( 배치크기, 데이터-shape )\n",
        "print( input.shape )\n",
        "\n",
        "output = base_model.predict(input)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_M9ZhkSSLog",
        "outputId": "84d17db2-76f8-4495-8dd9-b57b26058f10"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 256, 3)\n",
            "(1, 256, 256, 3)\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "(1, 8, 8, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력데이터를 받아서 사전 학습 모델에서 처리한 특성 맵 생성 함수 만들기\n",
        "def get_features_and_labels_by_vgg16_base_model(dataset, cnt):\n",
        "  all_features = []\n",
        "  all_labels = []\n",
        "\n",
        "  for idx, (images, labels) in enumerate(dataset):\n",
        "    # features = base_model.predict(images)\n",
        "    processed_images = tf_keras.applications.vgg16.preprocess_input(images) # 기본 vgg16 모델에 맞게 데이터 변형\n",
        "    features = base_model.predict(processed_images) # 합성곱층을 거친 데이터\n",
        "    all_features.append(features)\n",
        "    all_labels.append(labels)\n",
        "\n",
        "    if idx == cnt:\n",
        "      break\n",
        "\n",
        "  return np.concatenate(all_features), np.concatenate(all_labels)\n"
      ],
      "metadata": {
        "id": "_WS4Q4bOZ8Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련데이터, 검증데이터, 테스트 데이터에 대해 사전 학습 모델에서 처리한 특성 맵 생성\n",
        "train_features, train_labels = get_features_and_labels_by_vgg16_base_model(datasets[0], 100)\n",
        "validation_features, validation_labels = get_features_and_labels_by_vgg16_base_model(datasets[1], 50)\n",
        "test_features, test_labels = get_features_and_labels_by_vgg16_base_model(datasets[2], 50)"
      ],
      "metadata": {
        "id": "aYKIjLT6l5EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features.shape, train_labels.shape, test_features.shape, test_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRgJt8QYljAb",
        "outputId": "f723b687-d432-4f6d-a676-20b94b1b6117"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3200, 8, 8, 512), (3200,), (1608, 8, 8, 512), (1608,))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 학습 모델을 통해 만든 특성맵 데이터를 사용하는 판별 모델 구성\n",
        "\n",
        "# model1 = tf_keras.Sequential([\n",
        "#     tf_keras.layers.Input(shape=(8, 8, 512)),\n",
        "#     tf_keras.layers.Flatten(),\n",
        "#     tf_keras.layers.Dense(units=256, activation=\"relu\"),\n",
        "#     tf_keras.layers.Dense(units=1, activation=\"sigmoid\")\n",
        "# ])\n",
        "\n",
        "input = tf_keras.layers.Input(shape=(8, 8, 512))\n",
        "x = tf_keras.layers.Flatten()(input)\n",
        "x = tf_keras.layers.Dense(units=256, activation=\"relu\")(x)\n",
        "output = tf_keras.layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model1 = tf_keras.models.Model(input, output)"
      ],
      "metadata": {
        "id": "-k41t9lfmOtu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(optimizer=\"adam\",\n",
        "               loss=\"binary_crossentropy\",\n",
        "               metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "EKuUcKgtnVik"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model1.fit(train_features, train_labels,\n",
        "                     batch_size=32,\n",
        "                     epochs=20,\n",
        "                     validation_data=(validation_features, validation_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twx1TAuBoDxM",
        "outputId": "faf774ed-620f-4210-c895-e243a7942fbb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "100/100 [==============================] - 4s 16ms/step - loss: 1.2329 - accuracy: 0.4931 - val_loss: 0.6853 - val_accuracy: 0.5299\n",
            "Epoch 2/20\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.6903 - accuracy: 0.5484 - val_loss: 0.6832 - val_accuracy: 0.4975\n",
            "Epoch 3/20\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.6918 - accuracy: 0.5381 - val_loss: 0.7040 - val_accuracy: 0.5193\n",
            "Epoch 4/20\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.6973 - accuracy: 0.5263 - val_loss: 0.6783 - val_accuracy: 0.5143\n",
            "Epoch 5/20\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.6817 - accuracy: 0.5603 - val_loss: 0.6671 - val_accuracy: 0.5920\n",
            "Epoch 6/20\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.6671 - accuracy: 0.5913 - val_loss: 0.7318 - val_accuracy: 0.4950\n",
            "Epoch 7/20\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.6749 - accuracy: 0.5763 - val_loss: 0.6565 - val_accuracy: 0.6275\n",
            "Epoch 8/20\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.6702 - accuracy: 0.5784 - val_loss: 0.6540 - val_accuracy: 0.6412\n",
            "Epoch 9/20\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.6670 - accuracy: 0.5809 - val_loss: 0.6702 - val_accuracy: 0.5765\n",
            "Epoch 10/20\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.6662 - accuracy: 0.5806 - val_loss: 0.7022 - val_accuracy: 0.5000\n",
            "Epoch 11/20\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.6613 - accuracy: 0.5916 - val_loss: 0.6473 - val_accuracy: 0.6611\n",
            "Epoch 12/20\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.6616 - accuracy: 0.5853 - val_loss: 0.6539 - val_accuracy: 0.6182\n",
            "Epoch 13/20\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.6657 - accuracy: 0.5838 - val_loss: 0.6613 - val_accuracy: 0.5796\n",
            "Epoch 14/20\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.6526 - accuracy: 0.6000 - val_loss: 0.6605 - val_accuracy: 0.5821\n",
            "Epoch 15/20\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.6547 - accuracy: 0.5956 - val_loss: 0.6650 - val_accuracy: 0.5896\n",
            "Epoch 16/20\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.6535 - accuracy: 0.6194 - val_loss: 0.6544 - val_accuracy: 0.6113\n",
            "Epoch 17/20\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.6434 - accuracy: 0.6244 - val_loss: 0.6558 - val_accuracy: 0.6095\n",
            "Epoch 18/20\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.6494 - accuracy: 0.6053 - val_loss: 0.6406 - val_accuracy: 0.6468\n",
            "Epoch 19/20\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.6438 - accuracy: 0.6228 - val_loss: 0.6505 - val_accuracy: 0.6206\n",
            "Epoch 20/20\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.6440 - accuracy: 0.6150 - val_loss: 0.6820 - val_accuracy: 0.5578\n"
          ]
        }
      ]
    }
  ]
}