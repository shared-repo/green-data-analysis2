{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\miniconda3\\envs\\study-env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# base_dir = \"data-files/aclImdb\"\n",
    "base_dir = \"D:\\\\instructor-och\\\\data-files\\\\aclImdb\"\n",
    "train_dataset = tf_keras.utils.text_dataset_from_directory(base_dir + \"\\\\train\", batch_size=32)\n",
    "test_dataset = tf_keras.utils.text_dataset_from_directory(base_dir + \"\\\\test\", batch_size=32)\n",
    "review_only_dataset = train_dataset.map(lambda review, label: review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,) (32,)\n",
      "tf.Tensor(b\"... mainly because Ju-on 2 boasts an outrageous FORTY minutes' worth of material literally taken straight out of the first Ju-on - and when you consider that the sequel only runs for 76 minutes, that leaves you with 36 original minutes' worth of film. Ho-hum. I found that deeply irritating - as if viewers simply wouldn't remember the same stuff! - not to mention dull, having to watch it all over again.<br /><br />OK, that complaint aside, the byline for Ju-on 2 was that it was supposed to explain a lot of the unanswered questions from the first movie, which frankly, over 36 minutes, simply doesn't go far enough to making any kind of sense of the original's highly convoluted storyline.<br /><br />There are, however, some really nice new horror sequences which show how good the film might have been, had it had some time to develop; and some of the questions raised by the original - some, but not all - are answered.<br /><br />So in conclusion - if you loved the first original movie and want to see some further developments on the story, go for it - but just remember to keep your remote control to hand with your finger on the fast-forward button for forty minutes.\", shape=(), dtype=string) tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataset:\n",
    "    print(X.shape, y.shape)\n",
    "    print(X[0], y[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 -> 숫자 인코딩 (BOW, 단어번호벡터, ...)\n",
    "text_vectorizer = tf_keras.layers.TextVectorization(max_tokens=20000,  # 사용할 단어 갯수\n",
    "                                                    output_mode=\"int\", # 출력은 단어 사전의 번호\n",
    "                                                    output_sequence_length=300) # 각 문장의 길이\n",
    "text_vectorizer.adapt(review_only_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 300)\n",
      "tf.Tensor(\n",
      "[[  196   744  2069 ...     0     0     0]\n",
      " [  241   173 15476 ...     0     0     0]\n",
      " [   10   418    78 ...     0     0     0]\n",
      " ...\n",
      " [   51    10    86 ...   460    37     7]\n",
      " [   15    10  1751 ...     0     0     0]\n",
      " [   10   321   761 ...     0     0     0]], shape=(32, 300), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataset:    \n",
    "    d = text_vectorizer(X)\n",
    "    print(d.shape)    \n",
    "    print( text_vectorizer(X) )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 값 -> 압축된 단어 벡터 ( 과정 학습 ), 한 행의 문장 -> 여러 행의 단어 벡터\n",
    "# input_dim : 총 단어 갯수, output_dim : 한 단어를 표현하는 vector\n",
    "input = tf_keras.layers.Input(shape=(None,))\n",
    "output = tf_keras.layers.Embedding(input_dim=20000, output_dim=100)(input)\n",
    "\n",
    "embedding_model = tf_keras.models.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = train_dataset.map(lambda review, label: (text_vectorizer(review), label) )\n",
    "\n",
    "# for X, y in input_dataset:\n",
    "#     print(X, y)\n",
    "\n",
    "all_data = []\n",
    "for x, y in input_dataset.as_numpy_iterator(): # tensorflow tensor -> numpy ndarray\n",
    "    all_data.append(x)\n",
    "\n",
    "review_only_input_dataset = np.concatenate(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[11492,   601,   917, ...,    78,     2,  3866],\n",
       "       [   51,    10,    14, ...,    30,   533,  1825],\n",
       "       [  696,     3,  4723, ...,     0,     0,     0],\n",
       "       [   10,    14, 10124, ...,     0,     0,     0],\n",
       "       [  330, 10987,   149, ...,     0,     0,     0]], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(review_only_input_dataset.shape)\n",
    "review_only_input_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치크기 * 단어 갯수 -> 배치크기 * 단어 갯수 * 단어표현크기\n",
    "embeded_dataset = embedding_model(review_only_input_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([25000, 300, 100])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_dataset.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
