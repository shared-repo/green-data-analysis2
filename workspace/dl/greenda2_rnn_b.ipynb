{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\miniconda3\\envs\\study-env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# base_dir = \"data-files/aclImdb\"\n",
    "base_dir = \"D:\\\\instructor-och\\\\data-files\\\\aclImdb\"\n",
    "train_dataset = tf_keras.utils.text_dataset_from_directory(base_dir + \"\\\\train\", batch_size=32)\n",
    "test_dataset = tf_keras.utils.text_dataset_from_directory(base_dir + \"\\\\test\", batch_size=32)\n",
    "review_only_dataset = train_dataset.map(lambda review, label: review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,) (32,)\n",
      "tf.Tensor(b\"... mainly because Ju-on 2 boasts an outrageous FORTY minutes' worth of material literally taken straight out of the first Ju-on - and when you consider that the sequel only runs for 76 minutes, that leaves you with 36 original minutes' worth of film. Ho-hum. I found that deeply irritating - as if viewers simply wouldn't remember the same stuff! - not to mention dull, having to watch it all over again.<br /><br />OK, that complaint aside, the byline for Ju-on 2 was that it was supposed to explain a lot of the unanswered questions from the first movie, which frankly, over 36 minutes, simply doesn't go far enough to making any kind of sense of the original's highly convoluted storyline.<br /><br />There are, however, some really nice new horror sequences which show how good the film might have been, had it had some time to develop; and some of the questions raised by the original - some, but not all - are answered.<br /><br />So in conclusion - if you loved the first original movie and want to see some further developments on the story, go for it - but just remember to keep your remote control to hand with your finger on the fast-forward button for forty minutes.\", shape=(), dtype=string) tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataset:\n",
    "    print(X.shape, y.shape)\n",
    "    print(X[0], y[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 -> 숫자 인코딩 (BOW, 단어번호벡터, ...)\n",
    "text_vectorizer = tf_keras.layers.TextVectorization(max_tokens=20000,  # 사용할 단어 갯수\n",
    "                                                    output_mode=\"int\", # 출력은 단어 사전의 번호\n",
    "                                                    output_sequence_length=300) # 각 문장의 길이\n",
    "text_vectorizer.adapt(review_only_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 300)\n",
      "tf.Tensor(\n",
      "[[  1 216  18 ...   0   0   0]\n",
      " [ 32  10  69 ...   0   0   0]\n",
      " [ 11  18  14 ...   0   0   0]\n",
      " ...\n",
      " [ 86 127  10 ...   0   0   0]\n",
      " [ 74  10  67 ...   0   0   0]\n",
      " [ 86   5  32 ...   0   0   0]], shape=(32, 300), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataset:    \n",
    "    d = text_vectorizer(X)\n",
    "    print(d.shape)    \n",
    "    print( text_vectorizer(X) )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 값 -> 압축된 단어 벡터 ( 과정 학습 ), 한 행의 문장 -> 여러 행의 단어 벡터\n",
    "# input_dim : 총 단어 갯수, output_dim : 한 단어를 표현하는 vector\n",
    "input = tf_keras.layers.Input(shape=(None,))\n",
    "output = tf_keras.layers.Embedding(input_dim=20000, output_dim=100)(input)\n",
    "\n",
    "embedding_model = tf_keras.models.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터셋 전체에대해 text_vectorization 적용\n",
    "input_dataset = train_dataset.map(lambda review, label: (text_vectorizer(review), label) )\n",
    "\n",
    "for X, y in input_dataset:\n",
    "    print(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for x, y in input_dataset.as_numpy_iterator(): # tensorflow tensor -> numpy ndarray\n",
    "    # print(type(x))\n",
    "    # break\n",
    "    all_data.append(x)\n",
    "\n",
    "review_only_input_dataset = np.concatenate(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   11,    18,    14, ...,     0,     0,     0],\n",
       "       [16226, 15814,     1, ...,     0,     0,     0],\n",
       "       [  140,    43,  4135, ...,     0,     0,     0],\n",
       "       [ 1424,   887,    15, ..., 11254,   380,    11],\n",
       "       [ 5785,     7,     4, ...,     0,     0,     0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(review_only_input_dataset.shape)\n",
    "review_only_input_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치크기 * 단어 갯수 -> 배치크기 * 단어 갯수 * 단어표현크기\n",
    "embeded_dataset = embedding_model(review_only_input_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([25000, 300, 100])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300, 100), dtype=float32, numpy=\n",
       "array([[[ 0.01187342, -0.01256521,  0.00752597, ..., -0.04648696,\n",
       "         -0.01624123,  0.02742126],\n",
       "        [-0.00443424,  0.01624299, -0.0347303 , ..., -0.04067074,\n",
       "          0.02249147,  0.01066671],\n",
       "        [ 0.03618124, -0.00179081, -0.02350274, ...,  0.04206263,\n",
       "          0.01330247, -0.02644373],\n",
       "        ...,\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ],\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ],\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ]],\n",
       "\n",
       "       [[-0.01873448,  0.00226523, -0.00522152, ...,  0.04537462,\n",
       "         -0.04577348, -0.02152989],\n",
       "        [ 0.02371925,  0.04375834,  0.04883577, ..., -0.0227849 ,\n",
       "         -0.04484495,  0.03489175],\n",
       "        [ 0.00600901,  0.00932712, -0.00301258, ...,  0.04149136,\n",
       "          0.01192706, -0.00964012],\n",
       "        ...,\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ],\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ],\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ]],\n",
       "\n",
       "       [[ 0.02606598, -0.02443822, -0.01665219, ..., -0.0435681 ,\n",
       "         -0.00669745,  0.02504469],\n",
       "        [-0.04657614,  0.01870624, -0.03887594, ...,  0.00439738,\n",
       "          0.0427992 , -0.03128426],\n",
       "        [ 0.01370088, -0.0172342 ,  0.0096597 , ..., -0.01302658,\n",
       "         -0.005112  , -0.00385166],\n",
       "        ...,\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ],\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ],\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.01187342, -0.01256521,  0.00752597, ..., -0.04648696,\n",
       "         -0.01624123,  0.02742126],\n",
       "        [-0.01862016, -0.0046312 , -0.04986938, ..., -0.01377982,\n",
       "         -0.02613719,  0.03650519],\n",
       "        [-0.01934024,  0.02644337, -0.0240042 , ...,  0.03197462,\n",
       "          0.04157684,  0.04007107],\n",
       "        ...,\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ],\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ],\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ]],\n",
       "\n",
       "       [[ 0.01187342, -0.01256521,  0.00752597, ..., -0.04648696,\n",
       "         -0.01624123,  0.02742126],\n",
       "        [-0.00443424,  0.01624299, -0.0347303 , ..., -0.04067074,\n",
       "          0.02249147,  0.01066671],\n",
       "        [ 0.03618124, -0.00179081, -0.02350274, ...,  0.04206263,\n",
       "          0.01330247, -0.02644373],\n",
       "        ...,\n",
       "        [-0.02247776,  0.03976652, -0.03623713, ..., -0.02278271,\n",
       "          0.00378985,  0.01596   ],\n",
       "        [ 0.02483381,  0.04219725,  0.04096022, ..., -0.02636654,\n",
       "          0.0199675 , -0.03694464],\n",
       "        [ 0.03859476,  0.01992847, -0.02705431, ..., -0.0424519 ,\n",
       "          0.01478341,  0.01323135]],\n",
       "\n",
       "       [[ 0.00982578, -0.02346047,  0.04138725, ..., -0.01973099,\n",
       "         -0.01656281, -0.00578596],\n",
       "        [-0.01185068, -0.04461446,  0.01962384, ..., -0.03061993,\n",
       "         -0.01886628, -0.03792538],\n",
       "        [-0.01022919,  0.03341088,  0.00255647, ..., -0.00035974,\n",
       "         -0.01337104, -0.04366331],\n",
       "        ...,\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ],\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ],\n",
       "        [ 0.04079923, -0.04126881,  0.0022082 , ..., -0.02891971,\n",
       "          0.0403269 ,  0.0318062 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf_keras.layers.Input(shape=(None,))\n",
    "x = tf_keras.layers.Embedding(input_dim=20000, output_dim=100)(input)\n",
    "x = tf_keras.layers.LSTM(16)(x)\n",
    "output = tf_keras.layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "model = tf_keras.models.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\miniconda3\\envs\\study-env\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\ProgramData\\miniconda3\\envs\\study-env\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "782/782 [==============================] - 62s 78ms/step - loss: 0.6791 - accuracy: 0.5389\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 61s 78ms/step - loss: 0.5994 - accuracy: 0.7059\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 63s 81ms/step - loss: 0.6273 - accuracy: 0.6536\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.5297 - accuracy: 0.7556\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.4830 - accuracy: 0.7877\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.4533 - accuracy: 0.8055\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.4837 - accuracy: 0.7572\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.4558 - accuracy: 0.7837\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 0.4679 - accuracy: 0.7957\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4135 - accuracy: 0.8275\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(input_dataset, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
