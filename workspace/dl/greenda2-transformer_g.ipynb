{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window os\n",
    "# !pip install \"git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf\"\n",
    "# Other os\n",
    "# !pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비 1\n",
    "daum_movie_review_df = pd.read_csv('data-files/daum_movie_review.csv')\n",
    "daum_movie_review_df.head()\n",
    "daum_X = daum_movie_review_df['review'].to_list()\n",
    "daum_y = daum_movie_review_df['rating'].map(lambda r: 1 if r > 5 else 0).to_list()\n",
    "daum_train_X, daum_test_X, daum_train_y, daum_test_y = \\\n",
    "    train_test_split(daum_X, daum_y, stratify=daum_y, random_state=42)\n",
    "daum_train_X, daum_valid_X, daum_train_y, daum_valid_y = \\\n",
    "    train_test_split(daum_train_X, daum_train_y, stratify=daum_train_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비 2\n",
    "naver_movie_train_review_df = pd.read_csv('data-files/ratings_train.txt', sep='\\t')\n",
    "naver_movie_test_review_df = pd.read_csv('data-files/ratings_test.txt', sep='\\t')\n",
    "\n",
    "naver_train_X = naver_movie_train_review_df['document'].to_list()\n",
    "naver_train_y = naver_movie_train_review_df['label'].to_list()\n",
    "naver_test_X = naver_movie_test_review_df['document'].to_list()\n",
    "naver_test_y = naver_movie_test_review_df['label'].to_list()\n",
    "naver_train_X, naver_valid_X, naver_train_y, naver_valid_y = \\\n",
    "    train_test_split(naver_train_X, naver_train_y, stratify=naver_train_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# daum_train_X.shape, daum_valid_X.shape, daum_test_X.shape\n",
    "type(daum_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55e0dbedfee47838bbedb0fc494ccab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5407fa88784d47ba8267da741bae9243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/371k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e310b182a94a1f844eaa893e511b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained(\"skt/kobert-base-v1\")\n",
    "\n",
    "daum_train_inputs = tokenizer(daum_train_X, truncation=True, padding=True, return_tensors='pt')\n",
    "daum_valid_inputs = tokenizer(daum_valid_X, truncation=True, padding=True, return_tensors='pt')\n",
    "daum_test_inputs = tokenizer(daum_test_X, truncation=True, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8282, 280]) torch.Size([3682, 289])\n",
      "torch.Size([280])\n"
     ]
    }
   ],
   "source": [
    "print( daum_train_inputs['input_ids'].shape, daum_test_inputs['input_ids'].shape )\n",
    "\n",
    "for item in daum_train_inputs.items():\n",
    "    print(item[1][2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = { k: torch.tensor(v[idx]) for k, v in self.inputs.items() }\n",
    "        input['label'] = torch.tensor(self.labels[idx])\n",
    "        return input\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "daum_train_dataset = ReviewDataset(daum_train_inputs, daum_train_y)\n",
    "daum_valid_dataset = ReviewDataset(daum_valid_inputs, daum_valid_y)\n",
    "daum_test_dataset = ReviewDataset(daum_test_inputs, daum_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      "torch.Size([280])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator.User -2023YNCQT\\AppData\\Local\\Temp\\ipykernel_17896\\2483146844.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = { k: torch.tensor(v[idx]) for k, v in self.inputs.items() }\n"
     ]
    }
   ],
   "source": [
    "for data in daum_train_dataset:\n",
    "    print(data.keys())\n",
    "    print(data['input_ids'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_name = \"cuda:0\" if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_name)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6934031761e24fc481e6c612927eea29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/535 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Administrator.User -2023YNCQT\\.cache\\huggingface\\hub\\models--skt--kobert-base-v1\\snapshots\\a9f5849fce18fb088f0cd0f9b29ec3f756958464\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"kobert_version\": 1.0,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9d1f73e65a440d81440e9e1c6ce4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/369M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Administrator.User -2023YNCQT\\.cache\\huggingface\\hub\\models--skt--kobert-base-v1\\snapshots\\a9f5849fce18fb088f0cd0f9b29ec3f756958464\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at skt/kobert-base-v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "class ReviewClassificationModel(nn.Module):\n",
    "    def __init__(self, pretrained_model, token_size, num_labels): \n",
    "        super(ReviewClassificationModel, self).__init__()\n",
    "        self.token_size = token_size\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.classifier = nn.Linear(self.token_size, self.num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.pretrained_model(**inputs)\n",
    "\n",
    "        bert_clf_token = outputs.last_hidden_state[:,0,:]\n",
    "        return self.classifier(bert_clf_token)\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"skt/kobert-base-v1\")\n",
    "model = ReviewClassificationModel(bert_model, num_labels=2, token_size=bert_model.config.hidden_size) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "daum_train_loader = DataLoader(daum_train_dataset, shuffle=True, batch_size=8)\n",
    "daum_valid_loader = DataLoader(daum_valid_dataset, shuffle=False, batch_size=8)\n",
    "daum_test_loader = DataLoader(daum_test_dataset, shuffle=False, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\study-env2\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator.User -2023YNCQT\\AppData\\Local\\Temp\\ipykernel_17896\\2483146844.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = { k: torch.tensor(v[idx]) for k, v in self.inputs.items() }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 100, elapsed time: 374.0636966228485, loss: 0.6320414543151855\n",
      "Epoch 1, batch 200, elapsed time: 733.6505360603333, loss: 0.5286116600036621\n",
      "Epoch 1, batch 300, elapsed time: 1099.0544118881226, loss: 0.49474114179611206\n",
      "Epoch 1, batch 400, elapsed time: 1461.7365765571594, loss: 0.45018264651298523\n",
      "Epoch 1, batch 500, elapsed time: 1825.981111049652, loss: 0.3763388395309448\n",
      "Epoch 1, batch 600, elapsed time: 2149.5333375930786, loss: 0.5203408002853394\n",
      "Epoch 1, batch 700, elapsed time: 2463.477724790573, loss: 0.41436439752578735\n",
      "Epoch 1, batch 800, elapsed time: 2770.7507317066193, loss: 0.4860530197620392\n",
      "Epoch 1, batch 900, elapsed time: 3076.558360815048, loss: 0.7401040196418762\n",
      "Epoch 1, batch 1000, elapsed time: 3380.1996953487396, loss: 0.38239946961402893\n",
      "Average loss for epoch 1: 0.4486391246318817\n",
      "Epoch 2, batch 100, elapsed time: 3795.690288066864, loss: 0.22836554050445557\n",
      "Epoch 2, batch 200, elapsed time: 4109.535093784332, loss: 0.10136257857084274\n",
      "Epoch 2, batch 300, elapsed time: 4419.432316064835, loss: 0.5964145660400391\n",
      "Epoch 2, batch 400, elapsed time: 4727.147182941437, loss: 0.44999462366104126\n",
      "Epoch 2, batch 500, elapsed time: 5041.108631849289, loss: 0.22052232921123505\n",
      "Epoch 2, batch 600, elapsed time: 5354.953600645065, loss: 0.2197991907596588\n",
      "Epoch 2, batch 700, elapsed time: 5668.746562719345, loss: 0.5440124869346619\n",
      "Epoch 2, batch 800, elapsed time: 5977.322646141052, loss: 0.4472176730632782\n",
      "Epoch 2, batch 900, elapsed time: 6283.586941242218, loss: 0.5460347533226013\n",
      "Epoch 2, batch 1000, elapsed time: 6588.431130409241, loss: 0.382299542427063\n",
      "Average loss for epoch 2: 0.47732973098754883\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "model.to(device)\n",
    "model.train() # 스위치 : 가중치 업데이터 활성화 모드\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5) \n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "start = time.time()\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    total_epoch_loss = 0\n",
    "    for step, batch in enumerate(daum_train_loader):\n",
    "        optim.zero_grad() # 기울기 초기화\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'} \n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, F.one_hot(labels, num_classes=2).float()) # 손실 계산\n",
    "\n",
    "        if (step+1) % 100 == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(f'Epoch {epoch+1}, batch {step+1}, elapsed time: {elapsed}, loss: {loss}')\n",
    "        total_epoch_loss += loss\n",
    "        loss.backward() # 기울기 계산\n",
    "        optim.step() # 가중치 업데이트\n",
    "    avg_epoch_loss = total_epoch_loss / len(daum_train_loader)\n",
    "    print(f'Average loss for epoch {epoch+1}: {avg_epoch_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
